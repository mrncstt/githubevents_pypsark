{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrncstt/githubevents_pypsark/blob/main/github_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGuEGnvKq7v4",
        "outputId": "93bc8b35-4e9d-472c-ce58-9158060a901e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488491 sha256=8dc9027edf7b94aabb3583c8581095fc9794c324d9da38c7458950b72bc7c6e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBUPhFMFrBi5",
        "outputId": "9d68b340-6f34-4914-8db8-de62666c5c02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (23.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-24.0-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 23.1.2\n",
            "    Uninstalling pip-23.1.2:\n",
            "      Successfully uninstalled pip-23.1.2\n",
            "Successfully installed pip-24.0\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e0g0lYl8rDNZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import calendar\n",
        "from zipfile import ZipFile\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from requests.exceptions import RequestException, HTTPError, ConnectionError, Timeout\n",
        "from tqdm import tqdm\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, to_date, when\n",
        "from IPython.display import FileLink, display\n",
        "import gzip\n",
        "import json\n",
        "import time\n",
        "import py4j\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "fwJiCre3amzb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5JaT529l14tv"
      },
      "outputs": [],
      "source": [
        "def create_directory(path):\n",
        "    \"\"\"\n",
        "    Creates a directory if it doesn't exist.\n",
        "\n",
        "    Parameters:\n",
        "    path (str): The path of the directory to be created.\n",
        "\n",
        "    Logs:\n",
        "    - Info: If the directory is created successfully or already exists.\n",
        "    - Error: If an error occurs during the directory creation process.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        path_obj = Path(path)\n",
        "        if not path_obj.exists():\n",
        "            path_obj.mkdir(parents=True, exist_ok=True)\n",
        "            logging.info(f\"Directory '{path}' created successfully.\")\n",
        "        else:\n",
        "            logging.info(f\"Directory '{path}' already exists.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred while creating the directory '{path}': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0SXCRpdK16Bu"
      },
      "outputs": [],
      "source": [
        "def download_file(url, output_file):\n",
        "    \"\"\"\n",
        "    Downloads a file from a URL to the specified output path.\n",
        "\n",
        "    Parameters:\n",
        "    url (str): The URL of the file to be downloaded.\n",
        "    output_file (str): The path where the downloaded file will be saved.\n",
        "\n",
        "    Logs:\n",
        "    - Info: If the file is downloaded successfully.\n",
        "    - Error: If an error occurs during the download process, including HTTP errors, connection errors, and timeouts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with requests.get(url, stream=True, timeout=10) as response:\n",
        "            response.raise_for_status()\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            block_size = 1024\n",
        "            t = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "\n",
        "            with open(output_file, 'wb') as f:\n",
        "                for data in response.iter_content(block_size):\n",
        "                    t.update(len(data))\n",
        "                    f.write(data)\n",
        "            t.close()\n",
        "\n",
        "        logging.info(f\"Downloaded - {url}\\nPath - {output_file}\")\n",
        "    except (requests.HTTPError, requests.ConnectionError, requests.Timeout) as e:\n",
        "        logging.error(f\"Error occurred while downloading {url}: {e}\")\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"Request error occurred while downloading {url}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-IwMEgGjH6BB"
      },
      "outputs": [],
      "source": [
        "def download_github_events(year, month, output_dir):\n",
        "    \"\"\"\n",
        "    Downloads GitHub event data for the specified year and month to the given output directory.\n",
        "\n",
        "    Parameters:\n",
        "    year (int): The year of the event data to download.\n",
        "    month (int): The month of the event data to download.\n",
        "    output_dir (str): The directory where the downloaded data will be saved.\n",
        "\n",
        "    Creates directories for each day and hour if they don't exist, and downloads the data files. Uses parallel processing to speed up the download process.\n",
        "    \"\"\"\n",
        "    base_url = \"https://data.gharchive.org/\"\n",
        "    create_directory(output_dir)\n",
        "\n",
        "    num_days = calendar.monthrange(year, month)[1]\n",
        "\n",
        "    def download_day_hour(day, hour):\n",
        "        \"\"\"Downloads the GitHub event data for a specific day and hour.\"\"\"\n",
        "        day_dir = os.path.join(output_dir, f\"{year}-{month:02d}-{day:02d}\")\n",
        "        create_directory(day_dir)\n",
        "\n",
        "        url = f\"{base_url}{year}-{month:02d}-{day:02d}-{hour}.json.gz\"\n",
        "        output_file = os.path.join(day_dir, f\"{year}-{month:02d}-{day:02d}-{hour}.json.gz\")\n",
        "        if not os.path.exists(output_file):\n",
        "            logging.info(f\"Downloading {url} to {output_file}\")\n",
        "            download_file(url, output_file)\n",
        "        else:\n",
        "            logging.info(f\"File already exists: {output_file}\")\n",
        "\n",
        "    # Use ThreadPoolExecutor for parallel downloads\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        for day in range(1, num_days + 1):\n",
        "            for hour in range(24):\n",
        "                executor.submit(download_day_hour, day, hour)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ll4DmmMw-P-t"
      },
      "outputs": [],
      "source": [
        "def verify_downloaded_files(year, month, output_dir):\n",
        "    \"\"\"\n",
        "    Verifies that all expected files for a given month and year have been downloaded.\n",
        "\n",
        "    This function checks for the presence of JSON files compressed with gzip (.json.gz)\n",
        "    for every hour of every day in the specified month and year. It assumes the files are\n",
        "    organized in a directory structure where each day has its own subdirectory named\n",
        "    in the format 'YYYY-MM-DD', and each file is named in the format 'YYYY-MM-DD-HH.json.gz'.\n",
        "\n",
        "    Parameters:\n",
        "    year (int): The year for which to verify the files.\n",
        "    month (int): The month for which to verify the files (1-12).\n",
        "    output_dir (str): The base directory where the files are expected to be located.\n",
        "\n",
        "    Prints:\n",
        "    A list of missing files if any are not found, otherwise a confirmation message that all files are present.\n",
        "    \"\"\"\n",
        "    missing_files = []\n",
        "    num_days = calendar.monthrange(year, month)[1]\n",
        "\n",
        "    for day in range(1, num_days + 1):\n",
        "        for hour in range(24):\n",
        "            expected_file = os.path.join(output_dir, f\"{year}-{month:02d}-{day:02d}\", f\"{year}-{month:02d}-{day:02d}-{hour}.json.gz\")\n",
        "            if not os.path.exists(expected_file):\n",
        "                missing_files.append(expected_file)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"Some files are missing ({len(missing_files)} files):\")\n",
        "        for file in missing_files:\n",
        "            print(file)\n",
        "    else:\n",
        "        print(\"All files have been downloaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "zljwR1eU-RsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95220c4b-f04a-4cd6-ffd1-f8b237bb938b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r100%|██████████| 117M/117M [00:02<00:00, 52.1MiB/s]\n"
          ]
        }
      ],
      "source": [
        "year = 2024\n",
        "month = 5\n",
        "output_dir = '/path/to/github_events/downloaded'\n",
        "output_directory = '/path/to/github_events/processed'\n",
        "zip_directory = '/path/to/github_events/zipped'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 998
        },
        "id": "_p9SGlCeGthb",
        "outputId": "30d3a948-cc78-41b7-d4bc-6ec3503e11ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 105M/105M [00:04<00:00, 22.7MiB/s]\n",
            "100%|██████████| 105M/105M [00:03<00:00, 27.3MiB/s]\n",
            "100%|██████████| 95.8M/95.8M [00:02<00:00, 37.2MiB/s]\n",
            "100%|██████████| 90.1M/90.1M [00:01<00:00, 48.1MiB/s]\n",
            "100%|██████████| 85.4M/85.4M [00:02<00:00, 35.5MiB/s]\n",
            "100%|██████████| 98.6M/98.6M [00:03<00:00, 30.5MiB/s]\n",
            "100%|██████████| 89.7M/89.7M [00:02<00:00, 35.3MiB/s]\n",
            "100%|██████████| 95.9M/95.9M [00:02<00:00, 36.5MiB/s]\n",
            "100%|██████████| 109M/109M [00:03<00:00, 33.2MiB/s]\n",
            "100%|██████████| 126M/126M [00:02<00:00, 47.6MiB/s]\n",
            "100%|██████████| 111M/111M [00:01<00:00, 55.6MiB/s]\n",
            "100%|██████████| 113M/113M [00:02<00:00, 38.5MiB/s]\n",
            "100%|██████████| 117M/117M [00:02<00:00, 58.4MiB/s]\n",
            "100%|██████████| 125M/125M [00:03<00:00, 40.2MiB/s]\n",
            "100%|██████████| 139M/139M [00:02<00:00, 48.8MiB/s]\n",
            "100%|██████████| 136M/136M [00:03<00:00, 44.9MiB/s]\n",
            "100%|██████████| 135M/135M [00:02<00:00, 63.1MiB/s]\n",
            "100%|██████████| 135M/135M [00:03<00:00, 43.9MiB/s]\n",
            "100%|██████████| 127M/127M [00:02<00:00, 50.2MiB/s]\n",
            "100%|██████████| 132M/132M [00:02<00:00, 59.8MiB/s]\n",
            "100%|██████████| 136M/136M [00:02<00:00, 63.3MiB/s]\n",
            "100%|██████████| 121M/121M [00:03<00:00, 38.9MiB/s]\n",
            "100%|██████████| 102M/102M [00:02<00:00, 44.5MiB/s] \n",
            "100%|██████████| 94.9M/94.9M [00:02<00:00, 37.6MiB/s]\n",
            "100%|██████████| 95.5M/95.5M [00:02<00:00, 37.8MiB/s]\n",
            "100%|██████████| 85.5M/85.5M [00:02<00:00, 41.5MiB/s]\n",
            "100%|██████████| 82.5M/82.5M [00:01<00:00, 55.1MiB/s]\n",
            "100%|██████████| 77.1M/77.1M [00:01<00:00, 43.5MiB/s]\n",
            "100%|██████████| 74.5M/74.5M [00:01<00:00, 53.6MiB/s]\n",
            "100%|██████████| 94.6M/94.6M [00:01<00:00, 49.9MiB/s]\n",
            "100%|██████████| 97.3M/97.3M [00:01<00:00, 60.0MiB/s]\n",
            "100%|██████████| 118M/118M [00:02<00:00, 57.2MiB/s]\n",
            "100%|██████████| 126M/126M [00:05<00:00, 23.2MiB/s]\n",
            "100%|██████████| 133M/133M [00:02<00:00, 45.3MiB/s]\n",
            "100%|██████████| 122M/122M [00:03<00:00, 33.9MiB/s]\n",
            "100%|██████████| 121M/121M [00:02<00:00, 48.1MiB/s]\n",
            "100%|██████████| 129M/129M [00:04<00:00, 30.6MiB/s]\n",
            "100%|██████████| 144M/144M [00:03<00:00, 45.2MiB/s]\n",
            "100%|██████████| 144M/144M [00:02<00:00, 47.9MiB/s]\n",
            "100%|██████████| 142M/142M [00:03<00:00, 43.7MiB/s]\n",
            " 37%|███▋      | 50.0M/136M [00:02<00:03, 24.3MiB/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-4c85271c2048>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdownload_github_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-c8168da72c3d>\u001b[0m in \u001b[0;36mdownload_github_events\u001b[0;34m(year, month, output_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Use ThreadPoolExecutor for parallel downloads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexecutor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mday\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_days\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mhour\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    236\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1096\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1097\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1117\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "download_github_events(year, month, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS6eMVk6EmoQ",
        "outputId": "7cdaab47-be79-46cc-c68b-7ade933ca58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2024-05-01, Files processed: 24\n",
            "Date: 2024-05-02, Files processed: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 39.2M/96.2M [00:00<00:01, 42.8MiB/s]"
          ]
        }
      ],
      "source": [
        "def read_files(output_dir):\n",
        "    \"\"\"\n",
        "    Reads all .json.gz files from the specified directory and groups them by day.\n",
        "\n",
        "    This function traverses through the specified directory and its subdirectories,\n",
        "    collects all .json.gz files, and groups them by their date, assuming that each subdirectory\n",
        "    is named in the format 'YYYY-MM-DD'. It returns a dictionary where the keys are dates and the\n",
        "    values are lists of file paths.\n",
        "\n",
        "    Parameters:\n",
        "    output_dir (str): The base directory where the files are expected to be located.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where the keys are dates (str) and the values are lists of file paths (str).\n",
        "\n",
        "    Prints:\n",
        "    A message indicating the number of files processed per day. If the directory does not exist,\n",
        "    it prints an error message.\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        print(f\"Directory {output_dir} does not exist.\")\n",
        "        return {}\n",
        "\n",
        "    files_by_day = {}\n",
        "\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".json.gz\"):\n",
        "                try:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_date = root.split('/')[-1]\n",
        "                    if file_date not in files_by_day:\n",
        "                        files_by_day[file_date] = []\n",
        "                    files_by_day[file_date].append(file_path)\n",
        "                except IndexError:\n",
        "                    print(f\"Skipping file with unexpected format: {file_path}\")\n",
        "\n",
        "    for date, files in files_by_day.items():\n",
        "        print(f\"Date: {date}, Files processed: {len(files)}\")\n",
        "\n",
        "    return files_by_day\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files_by_day = read_files(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19NlwSeQeZ2x",
        "outputId": "1afb6a00-840b-4c3f-ef2c-4f8b15198856"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2024-05-03, Files processed: 7\n",
            "Date: 2024-05-01, Files processed: 24\n",
            "Date: 2024-05-02, Files processed: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 59.6M/90.3M [00:01<00:00, 48.0MiB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_json(filepath):\n",
        "    \"\"\"\n",
        "    Checks if a gzip-compressed JSON file is valid.\n",
        "\n",
        "    This function attempts to open and read a gzip-compressed file line by line,\n",
        "    parsing each line as JSON. If all lines can be successfully parsed, the file\n",
        "    is considered valid. If any line fails to parse as JSON or if the file cannot\n",
        "    be read due to an EOFError, the file is considered invalid.\n",
        "\n",
        "    Parameters:\n",
        "    filepath (str): The path to the gzip-compressed JSON file.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the file is valid JSON, False otherwise.\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                json.loads(line)\n",
        "        return True\n",
        "    except (json.JSONDecodeError, EOFError):\n",
        "        return False"
      ],
      "metadata": {
        "id": "4DLsdT-PSSyi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_spark_session(max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Creates a Spark session with retry logic.\n",
        "\n",
        "    This function attempts to create a Spark session using the given number of retries and delay between retries.\n",
        "    If the Spark session cannot be created due to a Py4JNetworkError, it retries up to the specified maximum number of retries,\n",
        "    waiting for the specified delay between each attempt. If it fails to create the Spark session after the specified number of retries,\n",
        "    it raises an exception.\n",
        "\n",
        "    Parameters:\n",
        "    max_retries (int): The maximum number of retry attempts (default is 3).\n",
        "    retry_delay (int): The delay in seconds between retry attempts (default is 5 seconds).\n",
        "\n",
        "    Returns:\n",
        "    SparkSession: A SparkSession object if the session is successfully created.\n",
        "\n",
        "    Raises:\n",
        "    Exception: If the Spark session cannot be created after the specified number of retries.\n",
        "\n",
        "    \"\"\"\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            spark = SparkSession.builder.appName(\"GitHub\").getOrCreate()\n",
        "            return spark\n",
        "        except py4j.protocol.Py4JNetworkError as e:\n",
        "            retries += 1\n",
        "            print(f\"Retry {retries}/{max_retries} - Failed to create Spark session: {e}\")\n",
        "            time.sleep(retry_delay)\n",
        "    raise Exception(\"Failed to create Spark session after multiple retries\")"
      ],
      "metadata": {
        "id": "I3bs6-8kSUYp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rvEznQ_RDgLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df64b1bf-8ade-452e-e0f4-00b2926bdf27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 408k/53.9M [00:00<00:13, 4.05MiB/s]"
          ]
        }
      ],
      "source": [
        "def process_files(files_by_day, downloaded_directory, processed_directory, max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Processes JSON files grouped by day, performing aggregations and saving results.\n",
        "\n",
        "    This function processes JSON files grouped by their date, performing specific aggregations for repositories and users.\n",
        "    The results are saved as CSV and Parquet files. It includes retry logic for handling failures during Spark session creation\n",
        "    and file processing.\n",
        "\n",
        "    Parameters:\n",
        "    files_by_day (dict): A dictionary where the keys are dates (str) and the values are lists of file paths (str).\n",
        "    downloaded_directory (str): The directory where the downloaded JSON files are located.\n",
        "    processed_directory (str): The directory where the processed CSV and Parquet files will be saved.\n",
        "    max_retries (int): The maximum number of retry attempts for creating the Spark session and processing each file (default is 3).\n",
        "    retry_delay (int): The delay in seconds between retry attempts (default is 5 seconds).\n",
        "\n",
        "    Prints:\n",
        "    Progress and error messages, including retries and skipping of corrupted files.\n",
        "    \"\"\"\n",
        "    # Create necessary directories if they don't exist\n",
        "    os.makedirs(processed_directory, exist_ok=True)\n",
        "\n",
        "    # Process files for each day\n",
        "    for file_date, files in files_by_day.items():\n",
        "        print(f\"Processing files for {file_date}...\")\n",
        "\n",
        "        # Define the output paths\n",
        "        repo_output_csv = os.path.join(processed_directory, f\"repo_agg_{file_date}.csv\")\n",
        "        repo_output_parquet = os.path.join(processed_directory, f\"repo_agg_{file_date}.parquet\")\n",
        "        user_output_csv = os.path.join(processed_directory, f\"user_agg_{file_date}.csv\")\n",
        "        user_output_parquet = os.path.join(processed_directory, f\"user_agg_{file_date}.parquet\")\n",
        "\n",
        "        # Check if the output files already exist\n",
        "        if (os.path.exists(repo_output_csv) and os.path.exists(repo_output_parquet) and\n",
        "            os.path.exists(user_output_csv) and os.path.exists(user_output_parquet)):\n",
        "            print(f\"Output files for {file_date} already exist. Skipping processing.\")\n",
        "            continue\n",
        "\n",
        "        spark = create_spark_session(max_retries, retry_delay)\n",
        "\n",
        "        # Process each file individually to isolate any corrupted files\n",
        "        for file in files:\n",
        "            file_path = os.path.join(downloaded_directory, file)\n",
        "\n",
        "            # Validate JSON file before processing\n",
        "            if not is_valid_json(file_path):\n",
        "                print(f\"Skipping corrupted file {file_path}\")\n",
        "                continue\n",
        "\n",
        "            retries = 0\n",
        "            success = False\n",
        "\n",
        "            while retries < max_retries and not success:\n",
        "                try:\n",
        "                    # Read JSON file into DataFrame\n",
        "                    df = spark.read.json(file_path)\n",
        "\n",
        "                    # Extract necessary fields\n",
        "                    repo_df = df.select(to_date(col(\"created_at\")).alias(\"date\"),\n",
        "                                        col(\"repo.id\").alias(\"project_id\"),\n",
        "                                        col(\"repo.name\").alias(\"project_name\"),\n",
        "                                        col(\"type\"))\n",
        "\n",
        "                    user_df = df.select(to_date(col(\"created_at\")).alias(\"date\"),\n",
        "                                        col(\"actor.id\").alias(\"user_id\"),\n",
        "                                        col(\"actor.login\").alias(\"user_login\"),\n",
        "                                        col(\"type\"))\n",
        "\n",
        "                    # Aggregations for repository\n",
        "                    repo_agg = repo_df.groupBy(\"date\", \"project_id\", \"project_name\").agg(\n",
        "                        count(when(col(\"type\") == \"WatchEvent\", True)).alias(\"stars\"),\n",
        "                        count(when(col(\"type\") == \"ForkEvent\", True)).alias(\"forks\"),\n",
        "                        count(when(col(\"type\") == \"IssuesEvent\", True)).alias(\"issues\"),\n",
        "                        count(when(col(\"type\") == \"PullRequestEvent\", True)).alias(\"prs\")\n",
        "                    )\n",
        "\n",
        "                    # Aggregations for user\n",
        "                    user_agg = user_df.groupBy(\"date\", \"user_id\", \"user_login\").agg(\n",
        "                        count(when(col(\"type\") == \"WatchEvent\", True)).alias(\"starred_projects\"),\n",
        "                        count(when(col(\"type\") == \"IssuesEvent\", True)).alias(\"issues_created\"),\n",
        "                        count(when(col(\"type\") == \"PullRequestEvent\", True)).alias(\"prs_created\")\n",
        "                    )\n",
        "\n",
        "                    # Save the results for the current file\n",
        "                    repo_agg.write.csv(repo_output_csv, header=True, mode='overwrite')\n",
        "                    repo_agg.write.parquet(repo_output_parquet, mode='overwrite')\n",
        "                    user_agg.write.csv(user_output_csv, header=True, mode='overwrite')\n",
        "                    user_agg.write.parquet(user_output_parquet, mode='overwrite')\n",
        "\n",
        "                    print(f\"Processed and saved data for file {file}\")\n",
        "                    success = True\n",
        "                except py4j.protocol.Py4JJavaError as e:\n",
        "                    retries += 1\n",
        "                    print(f\"Retry {retries}/{max_retries} - Failed to process file {file} for {file_date} due to Java error: {e}\")\n",
        "                    time.sleep(retry_delay)\n",
        "                except py4j.protocol.Py4JNetworkError as e:\n",
        "                    retries += 1\n",
        "                    print(f\"Retry {retries}/{max_retries} - Failed to process file {file} for {file_date} due to network error: {e}\")\n",
        "                    time.sleep(retry_delay)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to process file {file} for {file_date}: {e}\")\n",
        "                    break\n",
        "\n",
        "        # Stop the Spark session\n",
        "        spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_files(files_by_day, output_dir, output_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jM5dg2XNQ28F",
        "outputId": "ca90d899-43b6-4dc0-c44b-f1df3220aa44"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 63%|██████▎   | 31.5M/49.7M [00:01<00:01, 14.8MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing files for 2024-05-03...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.7M/49.7M [00:03<00:00, 15.4MiB/s]\n",
            "100%|██████████| 55.6M/55.6M [00:02<00:00, 24.5MiB/s]\n",
            "100%|██████████| 56.9M/56.9M [00:01<00:00, 29.2MiB/s]\n",
            "100%|██████████| 57.1M/57.1M [00:01<00:00, 31.1MiB/s]\n",
            "100%|██████████| 64.3M/64.3M [00:02<00:00, 28.8MiB/s]\n",
            "100%|██████████| 64.3M/64.3M [00:03<00:00, 20.3MiB/s]\n",
            "100%|██████████| 60.9M/60.9M [00:03<00:00, 18.1MiB/s]\n",
            "100%|██████████| 64.7M/64.7M [00:03<00:00, 18.6MiB/s]\n",
            "100%|██████████| 69.6M/69.6M [00:03<00:00, 19.4MiB/s]\n",
            "100%|██████████| 68.1M/68.1M [00:03<00:00, 21.2MiB/s]\n",
            "100%|██████████| 71.2M/71.2M [00:03<00:00, 19.1MiB/s]\n",
            "100%|██████████| 73.3M/73.3M [00:03<00:00, 19.6MiB/s]\n",
            "100%|██████████| 72.2M/72.2M [00:03<00:00, 18.3MiB/s]\n",
            "100%|██████████| 67.0M/67.0M [00:02<00:00, 27.0MiB/s]\n",
            "100%|██████████| 68.6M/68.6M [00:02<00:00, 33.1MiB/s]\n",
            "100%|██████████| 64.0M/64.0M [00:02<00:00, 24.2MiB/s]\n",
            "100%|██████████| 61.4M/61.4M [00:01<00:00, 32.6MiB/s]\n",
            "100%|██████████| 55.4M/55.4M [00:03<00:00, 14.7MiB/s]\n",
            "100%|██████████| 49.0M/49.0M [00:02<00:00, 23.6MiB/s]\n",
            "100%|██████████| 44.5M/44.5M [00:02<00:00, 20.7MiB/s]\n",
            "100%|██████████| 56.8M/56.8M [00:01<00:00, 34.8MiB/s]\n",
            "100%|██████████| 51.5M/51.5M [00:01<00:00, 32.6MiB/s]\n",
            "100%|██████████| 47.6M/47.6M [00:01<00:00, 32.5MiB/s]\n",
            "100%|██████████| 48.4M/48.4M [00:02<00:00, 22.9MiB/s]\n",
            "100%|██████████| 47.6M/47.6M [00:01<00:00, 36.5MiB/s]\n",
            "100%|██████████| 84.7M/84.7M [00:02<00:00, 38.2MiB/s]\n",
            "100%|██████████| 51.4M/51.4M [00:02<00:00, 24.8MiB/s]\n",
            "100%|██████████| 89.9M/89.9M [00:03<00:00, 25.2MiB/s]\n",
            "100%|██████████| 56.7M/56.7M [00:03<00:00, 15.3MiB/s]\n",
            "100%|██████████| 67.9M/67.9M [00:02<00:00, 30.8MiB/s]\n",
            "100%|██████████| 69.1M/69.1M [00:01<00:00, 36.0MiB/s]\n",
            "100%|██████████| 66.2M/66.2M [00:01<00:00, 33.8MiB/s]\n",
            "100%|██████████| 73.8M/73.8M [00:05<00:00, 14.6MiB/s]\n",
            "100%|██████████| 69.6M/69.6M [00:02<00:00, 26.0MiB/s]\n",
            "100%|██████████| 74.6M/74.6M [00:02<00:00, 36.2MiB/s]\n",
            "100%|██████████| 78.9M/78.9M [00:02<00:00, 34.9MiB/s]\n",
            "100%|██████████| 74.7M/74.7M [00:03<00:00, 20.9MiB/s]\n",
            "100%|██████████| 72.7M/72.7M [00:04<00:00, 16.5MiB/s]\n",
            "100%|██████████| 73.4M/73.4M [00:02<00:00, 34.2MiB/s]\n",
            "100%|██████████| 73.8M/73.8M [00:01<00:00, 38.3MiB/s]\n",
            "100%|██████████| 67.6M/67.6M [00:02<00:00, 23.2MiB/s]\n",
            "100%|██████████| 62.9M/62.9M [00:04<00:00, 15.0MiB/s]\n",
            "100%|██████████| 56.1M/56.1M [00:01<00:00, 33.0MiB/s]\n",
            "100%|██████████| 52.2M/52.2M [00:01<00:00, 34.8MiB/s]\n",
            "100%|██████████| 85.8M/85.8M [00:02<00:00, 39.5MiB/s]\n",
            " 61%|██████    | 67.1M/110M [00:02<00:02, 21.2MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-03/2024-05-03-1.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 110M/110M [00:04<00:00, 22.4MiB/s]\n",
            "100%|██████████| 103M/103M [00:05<00:00, 19.2MiB/s]\n",
            "100%|██████████| 101M/101M [00:05<00:00, 20.0MiB/s] \n",
            "100%|██████████| 99.1M/99.1M [00:04<00:00, 19.9MiB/s]\n",
            "100%|██████████| 102M/102M [00:05<00:00, 18.9MiB/s]\n",
            "100%|██████████| 123M/123M [00:06<00:00, 18.4MiB/s]\n",
            "100%|██████████| 131M/131M [00:03<00:00, 36.1MiB/s]\n",
            "100%|██████████| 142M/142M [00:03<00:00, 38.3MiB/s]\n",
            "100%|██████████| 142M/142M [00:06<00:00, 23.3MiB/s]\n",
            "100%|██████████| 136M/136M [00:05<00:00, 23.1MiB/s]\n",
            "100%|██████████| 137M/137M [00:04<00:00, 28.9MiB/s]\n",
            "100%|██████████| 144M/144M [00:06<00:00, 21.5MiB/s]\n",
            "100%|██████████| 156M/156M [00:04<00:00, 35.6MiB/s]\n",
            "100%|██████████| 175M/175M [00:07<00:00, 24.5MiB/s]\n",
            "100%|██████████| 168M/168M [00:04<00:00, 34.0MiB/s]\n",
            "100%|██████████| 164M/164M [00:07<00:00, 22.8MiB/s]\n",
            "100%|██████████| 156M/156M [00:04<00:00, 31.1MiB/s]\n",
            " 50%|█████     | 79.1M/158M [00:04<00:06, 12.3MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-03/2024-05-03-5.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 158M/158M [00:08<00:00, 17.6MiB/s]\n",
            "100%|██████████| 149M/149M [00:09<00:00, 15.1MiB/s]\n",
            "100%|██████████| 143M/143M [00:07<00:00, 19.2MiB/s]\n",
            "100%|██████████| 131M/131M [00:06<00:00, 21.5MiB/s]\n",
            "100%|██████████| 112M/112M [00:03<00:00, 32.3MiB/s]\n",
            "100%|██████████| 92.1M/92.1M [00:02<00:00, 31.3MiB/s]\n",
            "100%|██████████| 90.6M/90.6M [00:03<00:00, 25.4MiB/s]\n",
            "100%|██████████| 86.3M/86.3M [00:03<00:00, 26.5MiB/s]\n",
            "100%|██████████| 79.6M/79.6M [00:03<00:00, 21.1MiB/s]\n",
            "100%|██████████| 81.1M/81.1M [00:02<00:00, 28.3MiB/s]\n",
            "100%|██████████| 77.2M/77.2M [00:03<00:00, 21.8MiB/s]\n",
            " 57%|█████▋    | 55.4M/96.7M [00:02<00:01, 38.1MiB/s]ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=45>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o13.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            " 61%|██████▏   | 59.3M/96.7M [00:02<00:01, 28.3MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to process file /path/to/github_events/downloaded/2024-05-03/2024-05-03-6.json.gz for 2024-05-03: An error occurred while calling o284.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 96.7M/96.7M [00:04<00:00, 22.1MiB/s]\n",
            "100%|██████████| 102M/102M [00:05<00:00, 17.0MiB/s]\n",
            "100%|██████████| 114M/114M [00:05<00:00, 19.7MiB/s]\n",
            "100%|██████████| 125M/125M [00:06<00:00, 19.2MiB/s]\n",
            "100%|██████████| 125M/125M [00:05<00:00, 21.5MiB/s]\n",
            "100%|██████████| 116M/116M [00:03<00:00, 33.0MiB/s]\n",
            "100%|██████████| 117M/117M [00:03<00:00, 29.4MiB/s]\n",
            "100%|██████████| 116M/116M [00:05<00:00, 21.1MiB/s]\n",
            "100%|██████████| 134M/134M [00:04<00:00, 27.9MiB/s]\n",
            "100%|██████████| 144M/144M [00:05<00:00, 25.8MiB/s]\n",
            "100%|██████████| 138M/138M [00:05<00:00, 26.5MiB/s]\n",
            "100%|██████████| 125M/125M [00:04<00:00, 28.6MiB/s]\n",
            "100%|██████████| 118M/118M [00:05<00:00, 22.3MiB/s]\n",
            "100%|██████████| 117M/117M [00:04<00:00, 27.2MiB/s]\n",
            "100%|██████████| 111M/111M [00:04<00:00, 26.3MiB/s]\n",
            "100%|██████████| 110M/110M [00:05<00:00, 19.9MiB/s]\n",
            "100%|██████████| 109M/109M [00:03<00:00, 32.1MiB/s]\n",
            "100%|██████████| 89.5M/89.5M [00:02<00:00, 31.9MiB/s]\n",
            "100%|██████████| 74.7M/74.7M [00:02<00:00, 29.2MiB/s]\n",
            "  4%|▍         | 3.33M/84.2M [00:00<00:09, 8.40MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-03/2024-05-03-4.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 84.2M/84.2M [00:04<00:00, 17.0MiB/s]\n",
            "100%|██████████| 76.4M/76.4M [00:03<00:00, 19.6MiB/s]\n",
            "100%|██████████| 74.3M/74.3M [00:03<00:00, 20.1MiB/s]\n",
            "100%|██████████| 70.9M/70.9M [00:03<00:00, 21.0MiB/s]\n",
            "100%|██████████| 68.8M/68.8M [00:03<00:00, 19.8MiB/s]\n",
            "100%|██████████| 85.7M/85.7M [00:04<00:00, 20.0MiB/s]\n",
            "100%|██████████| 87.2M/87.2M [00:03<00:00, 23.3MiB/s]\n",
            "100%|██████████| 102M/102M [00:03<00:00, 26.3MiB/s]\n",
            "100%|██████████| 114M/114M [00:02<00:00, 38.9MiB/s]\n",
            "100%|██████████| 116M/116M [00:02<00:00, 40.1MiB/s]\n",
            "100%|██████████| 105M/105M [00:05<00:00, 20.6MiB/s]\n",
            "100%|██████████| 109M/109M [00:03<00:00, 27.3MiB/s]\n",
            "100%|██████████| 108M/108M [00:04<00:00, 24.5MiB/s]\n",
            "100%|██████████| 121M/121M [00:05<00:00, 20.9MiB/s]\n",
            "100%|██████████| 127M/127M [00:04<00:00, 30.8MiB/s]\n",
            "100%|██████████| 121M/121M [00:04<00:00, 28.3MiB/s]\n",
            "100%|██████████| 115M/115M [00:05<00:00, 20.8MiB/s]\n",
            "100%|██████████| 136M/136M [00:06<00:00, 20.2MiB/s]\n",
            "100%|██████████| 106M/106M [00:04<00:00, 24.2MiB/s]\n",
            "100%|██████████| 107M/107M [00:03<00:00, 27.4MiB/s]\n",
            " 29%|██▉       | 29.4M/102M [00:07<00:13, 5.37MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-03/2024-05-03-2.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 102M/102M [00:12<00:00, 7.82MiB/s]\n",
            "100%|██████████| 100M/100M [00:07<00:00, 14.1MiB/s] \n",
            "100%|██████████| 95.2M/95.2M [00:05<00:00, 16.6MiB/s]\n",
            "100%|██████████| 75.6M/75.6M [00:04<00:00, 15.7MiB/s]\n",
            "100%|██████████| 80.8M/80.8M [00:03<00:00, 24.5MiB/s]\n",
            "100%|██████████| 77.4M/77.4M [00:04<00:00, 19.0MiB/s]\n",
            "100%|██████████| 76.3M/76.3M [00:03<00:00, 24.7MiB/s]\n",
            "100%|██████████| 71.2M/71.2M [00:02<00:00, 29.3MiB/s]\n",
            "100%|██████████| 66.7M/66.7M [00:02<00:00, 25.0MiB/s]\n",
            "100%|██████████| 79.5M/79.5M [00:04<00:00, 19.2MiB/s]\n",
            "100%|██████████| 81.4M/81.4M [00:03<00:00, 23.1MiB/s]\n",
            "100%|██████████| 88.4M/88.4M [00:03<00:00, 26.4MiB/s]\n",
            "100%|██████████| 122M/122M [00:05<00:00, 20.9MiB/s]\n",
            "100%|██████████| 123M/123M [00:04<00:00, 27.5MiB/s]\n",
            "100%|██████████| 122M/122M [00:04<00:00, 26.5MiB/s]\n",
            "100%|██████████| 121M/121M [00:05<00:00, 23.7MiB/s]\n",
            "100%|██████████| 113M/113M [00:04<00:00, 25.5MiB/s]\n",
            "100%|██████████| 112M/112M [00:05<00:00, 21.9MiB/s]\n",
            "100%|██████████| 117M/117M [00:06<00:00, 18.3MiB/s]\n",
            " 30%|███       | 36.3M/119M [00:01<00:03, 22.0MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-03/2024-05-03-3.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 119M/119M [00:05<00:00, 20.9MiB/s]\n",
            "100%|██████████| 113M/113M [00:05<00:00, 20.0MiB/s]\n",
            "100%|██████████| 108M/108M [00:05<00:00, 18.8MiB/s]\n",
            "100%|██████████| 105M/105M [00:05<00:00, 19.9MiB/s]\n",
            "100%|██████████| 97.8M/97.8M [00:05<00:00, 18.8MiB/s]\n",
            "100%|██████████| 95.4M/95.4M [00:03<00:00, 24.2MiB/s]\n",
            "100%|██████████| 89.2M/89.2M [00:04<00:00, 22.3MiB/s]\n",
            "100%|██████████| 81.3M/81.3M [00:02<00:00, 27.2MiB/s]\n",
            "100%|██████████| 76.6M/76.6M [00:02<00:00, 29.9MiB/s]\n",
            "100%|██████████| 80.1M/80.1M [00:02<00:00, 29.2MiB/s]\n",
            "100%|██████████| 84.4M/84.4M [00:04<00:00, 20.5MiB/s]\n",
            "100%|██████████| 81.3M/81.3M [00:03<00:00, 26.9MiB/s]\n",
            "100%|██████████| 75.5M/75.5M [00:02<00:00, 31.3MiB/s]\n",
            "100%|██████████| 73.6M/73.6M [00:02<00:00, 30.1MiB/s]\n",
            "100%|██████████| 86.1M/86.1M [00:02<00:00, 30.2MiB/s]\n",
            "100%|██████████| 90.3M/90.3M [00:03<00:00, 22.6MiB/s]\n",
            "100%|██████████| 95.3M/95.3M [00:02<00:00, 32.9MiB/s]\n",
            "100%|██████████| 107M/107M [00:03<00:00, 31.3MiB/s]\n",
            "100%|██████████| 103M/103M [00:06<00:00, 16.9MiB/s]\n",
            "100%|██████████| 102M/102M [00:02<00:00, 36.2MiB/s]\n",
            "100%|██████████| 96.4M/96.4M [00:02<00:00, 37.4MiB/s]\n",
            "100%|██████████| 104M/104M [00:02<00:00, 35.7MiB/s]\n",
            "100%|██████████| 111M/111M [00:04<00:00, 25.3MiB/s]\n",
            "100%|██████████| 112M/112M [00:03<00:00, 32.9MiB/s]\n",
            "100%|██████████| 110M/110M [00:03<00:00, 31.9MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-03/2024-05-03-0.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|█▉        | 20.9M/106M [00:00<00:02, 32.8MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing files for 2024-05-01...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 106M/106M [00:04<00:00, 22.6MiB/s]\n",
            "100%|██████████| 97.6M/97.6M [00:05<00:00, 19.4MiB/s]\n",
            "100%|██████████| 94.7M/94.7M [00:04<00:00, 20.2MiB/s]\n",
            "100%|██████████| 92.1M/92.1M [00:04<00:00, 19.2MiB/s]\n",
            "100%|██████████| 88.9M/88.9M [00:05<00:00, 17.3MiB/s]\n",
            "100%|██████████| 87.9M/87.9M [00:04<00:00, 19.0MiB/s]\n",
            "100%|██████████| 69.7M/69.7M [00:03<00:00, 17.5MiB/s]\n",
            "100%|██████████| 64.3M/64.3M [00:02<00:00, 27.1MiB/s]\n",
            "100%|██████████| 64.3M/64.3M [00:01<00:00, 36.0MiB/s]\n",
            "100%|██████████| 62.1M/62.1M [00:01<00:00, 35.8MiB/s]\n",
            "100%|██████████| 56.0M/56.0M [00:01<00:00, 33.7MiB/s]\n",
            "100%|██████████| 53.2M/53.2M [00:02<00:00, 19.9MiB/s]\n",
            "100%|██████████| 47.2M/47.2M [00:01<00:00, 23.6MiB/s]\n",
            "100%|██████████| 53.2M/53.2M [00:01<00:00, 38.6MiB/s]\n",
            "100%|██████████| 55.0M/55.0M [00:01<00:00, 34.6MiB/s]\n",
            "100%|██████████| 59.4M/59.4M [00:01<00:00, 35.2MiB/s]\n",
            "100%|██████████| 60.8M/60.8M [00:02<00:00, 23.6MiB/s]\n",
            "100%|██████████| 59.1M/59.1M [00:03<00:00, 19.1MiB/s]\n",
            "100%|██████████| 57.2M/57.2M [00:01<00:00, 32.6MiB/s]\n",
            "100%|██████████| 57.6M/57.6M [00:01<00:00, 33.8MiB/s]\n",
            "100%|██████████| 66.8M/66.8M [00:01<00:00, 38.1MiB/s]\n",
            "100%|██████████| 64.6M/64.6M [00:01<00:00, 37.5MiB/s]\n",
            "100%|██████████| 66.2M/66.2M [00:02<00:00, 24.5MiB/s]\n",
            "100%|██████████| 64.0M/64.0M [00:03<00:00, 20.6MiB/s]\n",
            "100%|██████████| 64.2M/64.2M [00:01<00:00, 36.6MiB/s]\n",
            "100%|██████████| 57.0M/57.0M [00:01<00:00, 40.2MiB/s]\n",
            "100%|██████████| 56.4M/56.4M [00:01<00:00, 34.0MiB/s]\n",
            "100%|██████████| 51.7M/51.7M [00:02<00:00, 20.5MiB/s]\n",
            "100%|██████████| 51.6M/51.6M [00:01<00:00, 26.8MiB/s]\n",
            "100%|██████████| 46.9M/46.9M [00:02<00:00, 20.0MiB/s]\n",
            "100%|██████████| 41.4M/41.4M [00:01<00:00, 29.4MiB/s]\n",
            "100%|██████████| 41.8M/41.8M [00:01<00:00, 41.2MiB/s]\n",
            "100%|██████████| 47.9M/47.9M [00:01<00:00, 35.5MiB/s]\n",
            "100%|██████████| 46.6M/46.6M [00:01<00:00, 34.6MiB/s]\n",
            "100%|██████████| 42.7M/42.7M [00:01<00:00, 27.2MiB/s]\n",
            "100%|██████████| 42.9M/42.9M [00:01<00:00, 28.9MiB/s]\n",
            "100%|██████████| 41.5M/41.5M [00:01<00:00, 21.1MiB/s]\n",
            "100%|██████████| 46.2M/46.2M [00:02<00:00, 22.8MiB/s]\n",
            "100%|██████████| 46.5M/46.5M [00:01<00:00, 36.7MiB/s]\n",
            "100%|██████████| 45.9M/45.9M [00:01<00:00, 33.3MiB/s]\n",
            " 17%|█▋        | 8.61M/49.8M [00:00<00:02, 19.4MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-01/2024-05-01-22.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 49.8M/49.8M [00:02<00:00, 20.2MiB/s]\n",
            "100%|██████████| 52.8M/52.8M [00:02<00:00, 20.3MiB/s]\n",
            "100%|██████████| 50.8M/50.8M [00:02<00:00, 20.7MiB/s]\n",
            "100%|██████████| 52.5M/52.5M [00:03<00:00, 15.8MiB/s]\n",
            "100%|██████████| 63.8M/63.8M [00:03<00:00, 19.6MiB/s]\n",
            "100%|██████████| 63.0M/63.0M [00:03<00:00, 19.3MiB/s]\n",
            "100%|██████████| 65.6M/65.6M [00:03<00:00, 21.1MiB/s]\n",
            "100%|██████████| 64.4M/64.4M [00:03<00:00, 18.9MiB/s]\n",
            "100%|██████████| 64.1M/64.1M [00:03<00:00, 18.9MiB/s]\n",
            "100%|██████████| 57.8M/57.8M [00:03<00:00, 18.7MiB/s]\n",
            "100%|██████████| 60.9M/60.9M [00:02<00:00, 20.6MiB/s]\n",
            "100%|██████████| 58.7M/58.7M [00:02<00:00, 20.1MiB/s]\n",
            "100%|██████████| 51.3M/51.3M [00:02<00:00, 19.3MiB/s]\n",
            "100%|██████████| 49.2M/49.2M [00:01<00:00, 35.7MiB/s]\n",
            "100%|██████████| 44.8M/44.8M [00:01<00:00, 34.2MiB/s]\n",
            "100%|██████████| 44.4M/44.4M [00:02<00:00, 22.1MiB/s]\n",
            "100%|██████████| 74.9M/74.9M [00:02<00:00, 25.6MiB/s]\n",
            "100%|██████████| 89.6M/89.6M [00:02<00:00, 32.3MiB/s]\n",
            "100%|██████████| 80.2M/80.2M [00:02<00:00, 34.2MiB/s]\n",
            "100%|██████████| 89.4M/89.4M [00:02<00:00, 40.1MiB/s]\n",
            "100%|██████████| 84.3M/84.3M [00:03<00:00, 24.1MiB/s]\n",
            "100%|██████████| 92.7M/92.7M [00:02<00:00, 31.0MiB/s]\n",
            "100%|██████████| 113M/113M [00:03<00:00, 35.2MiB/s]\n",
            "100%|██████████| 122M/122M [00:04<00:00, 27.4MiB/s]\n",
            "100%|██████████| 136M/136M [00:04<00:00, 28.2MiB/s]\n",
            "100%|██████████| 128M/128M [00:04<00:00, 27.5MiB/s]\n",
            "100%|██████████| 124M/124M [00:06<00:00, 20.6MiB/s]\n",
            "100%|██████████| 126M/126M [00:03<00:00, 33.2MiB/s]\n",
            "100%|██████████| 134M/134M [00:03<00:00, 39.5MiB/s]\n",
            "100%|██████████| 142M/142M [00:04<00:00, 28.9MiB/s]\n",
            "100%|██████████| 152M/152M [00:05<00:00, 28.3MiB/s]\n",
            "100%|██████████| 153M/153M [00:04<00:00, 36.4MiB/s]\n",
            "100%|██████████| 142M/142M [00:06<00:00, 23.5MiB/s]\n",
            "100%|██████████| 137M/137M [00:03<00:00, 34.9MiB/s]\n",
            "100%|██████████| 142M/142M [00:05<00:00, 26.8MiB/s]\n",
            "100%|██████████| 137M/137M [00:06<00:00, 20.7MiB/s]\n",
            "100%|██████████| 130M/130M [00:03<00:00, 35.8MiB/s]\n",
            "100%|██████████| 126M/126M [00:03<00:00, 35.4MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-01/2024-05-01-16.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 111M/111M [00:05<00:00, 19.5MiB/s]\n",
            "100%|██████████| 116M/116M [00:05<00:00, 19.5MiB/s]\n",
            "100%|██████████| 96.1M/96.1M [00:04<00:00, 20.3MiB/s]\n",
            "100%|██████████| 88.0M/88.0M [00:04<00:00, 19.3MiB/s]\n",
            "100%|██████████| 81.7M/81.7M [00:04<00:00, 19.1MiB/s]\n",
            "100%|██████████| 82.0M/82.0M [00:04<00:00, 19.6MiB/s]\n",
            "100%|██████████| 75.7M/75.7M [00:03<00:00, 25.1MiB/s]\n",
            "100%|██████████| 97.7M/97.7M [00:03<00:00, 32.2MiB/s]\n",
            "100%|██████████| 96.0M/96.0M [00:05<00:00, 18.0MiB/s]\n",
            "100%|██████████| 109M/109M [00:03<00:00, 31.6MiB/s]\n",
            "100%|██████████| 115M/115M [00:04<00:00, 28.6MiB/s]\n",
            "100%|██████████| 114M/114M [00:04<00:00, 25.8MiB/s]\n",
            "100%|██████████| 104M/104M [00:03<00:00, 29.8MiB/s]\n",
            "100%|██████████| 104M/104M [00:02<00:00, 34.8MiB/s]\n",
            "100%|██████████| 117M/117M [00:03<00:00, 31.9MiB/s]\n",
            "100%|██████████| 124M/124M [00:05<00:00, 22.5MiB/s]\n",
            "100%|██████████| 127M/127M [00:03<00:00, 34.7MiB/s]\n",
            "100%|██████████| 127M/127M [00:04<00:00, 28.9MiB/s]\n",
            "100%|██████████| 114M/114M [00:04<00:00, 23.0MiB/s]\n",
            "100%|██████████| 108M/108M [00:03<00:00, 32.0MiB/s]\n",
            "100%|██████████| 109M/109M [00:03<00:00, 27.3MiB/s]\n",
            "100%|██████████| 113M/113M [00:05<00:00, 21.1MiB/s]\n",
            "100%|██████████| 115M/115M [00:03<00:00, 33.9MiB/s]\n",
            "100%|██████████| 99.5M/99.5M [00:03<00:00, 27.0MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-01/2024-05-01-5.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 91.3M/91.3M [00:05<00:00, 16.6MiB/s]\n",
            "100%|██████████| 74.5M/74.5M [00:04<00:00, 18.0MiB/s]\n",
            "100%|██████████| 78.1M/78.1M [00:04<00:00, 19.1MiB/s]\n",
            "100%|██████████| 73.3M/73.3M [00:03<00:00, 20.5MiB/s]\n",
            "100%|██████████| 71.4M/71.4M [00:03<00:00, 19.8MiB/s]\n",
            "100%|██████████| 73.2M/73.2M [00:03<00:00, 19.9MiB/s]\n",
            "100%|██████████| 69.0M/69.0M [00:03<00:00, 18.2MiB/s]\n",
            "100%|██████████| 87.9M/87.9M [00:12<00:00, 7.16MiB/s]\n",
            "100%|██████████| 93.9M/93.9M [00:04<00:00, 20.7MiB/s]\n",
            "100%|██████████| 106M/106M [00:03<00:00, 29.9MiB/s]\n",
            "100%|██████████| 121M/121M [00:05<00:00, 22.6MiB/s]\n",
            "100%|██████████| 114M/114M [00:04<00:00, 26.6MiB/s]\n",
            "100%|██████████| 103M/103M [00:02<00:00, 34.5MiB/s]\n",
            "100%|██████████| 111M/111M [00:03<00:00, 32.9MiB/s]\n",
            "100%|██████████| 117M/117M [00:05<00:00, 22.9MiB/s]\n",
            "100%|██████████| 130M/130M [00:04<00:00, 27.9MiB/s]\n",
            "100%|██████████| 134M/134M [00:04<00:00, 32.7MiB/s]\n",
            "100%|██████████| 130M/130M [00:06<00:00, 21.3MiB/s]\n",
            "100%|██████████| 118M/118M [00:03<00:00, 32.7MiB/s]\n",
            "100%|██████████| 114M/114M [00:03<00:00, 32.8MiB/s]\n",
            "100%|██████████| 116M/116M [00:06<00:00, 18.3MiB/s]\n",
            "100%|██████████| 106M/106M [00:02<00:00, 36.8MiB/s]\n",
            "100%|██████████| 106M/106M [00:03<00:00, 31.6MiB/s]\n",
            "100%|██████████| 105M/105M [00:04<00:00, 21.8MiB/s]\n",
            "100%|██████████| 81.2M/81.2M [00:02<00:00, 30.9MiB/s]\n",
            "100%|██████████| 73.3M/73.3M [00:02<00:00, 36.2MiB/s]\n",
            "100%|██████████| 78.5M/78.5M [00:02<00:00, 36.3MiB/s]\n",
            "100%|██████████| 76.5M/76.5M [00:02<00:00, 35.0MiB/s]\n",
            " 77%|███████▋  | 57.3M/74.7M [00:02<00:00, 18.9MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed and saved data for file /path/to/github_events/downloaded/2024-05-01/2024-05-01-21.json.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 74.7M/74.7M [00:03<00:00, 20.8MiB/s]\n",
            "100%|██████████| 74.1M/74.1M [00:04<00:00, 18.3MiB/s]\n",
            "100%|██████████| 65.7M/65.7M [00:03<00:00, 19.5MiB/s]\n",
            "100%|██████████| 82.3M/82.3M [00:05<00:00, 14.4MiB/s]\n",
            "100%|██████████| 92.5M/92.5M [00:06<00:00, 14.4MiB/s]\n",
            "100%|██████████| 103M/103M [00:10<00:00, 10.2MiB/s]\n",
            "100%|██████████| 115M/115M [00:06<00:00, 17.7MiB/s]\n",
            "100%|██████████| 115M/115M [00:06<00:00, 17.0MiB/s]\n",
            "100%|██████████| 104M/104M [00:06<00:00, 16.8MiB/s]\n",
            "100%|██████████| 107M/107M [00:04<00:00, 22.5MiB/s]\n",
            " 39%|███▉      | 46.9M/119M [00:01<00:02, 35.6MiB/s]ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=45>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o655.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            " 45%|████▌     | 53.8M/119M [00:02<00:02, 28.4MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to process file /path/to/github_events/downloaded/2024-05-01/2024-05-01-17.json.gz for 2024-05-01: An error occurred while calling o1023.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 119M/119M [00:05<00:00, 22.3MiB/s]\n",
            "100%|██████████| 121M/121M [00:06<00:00, 18.7MiB/s]\n",
            "100%|██████████| 131M/131M [00:07<00:00, 18.0MiB/s]\n",
            "100%|██████████| 125M/125M [00:06<00:00, 18.6MiB/s]\n",
            "100%|██████████| 117M/117M [00:06<00:00, 18.6MiB/s]\n",
            " 72%|███████▏  | 88.8M/123M [00:05<00:04, 7.23MiB/s]ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "RuntimeError: reentrant call inside <_io.BufferedReader name=45>\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            "ERROR:root:Exception while sending command.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
            "    answer = smart_decode(self.stream.readline()[:-1])\n",
            "  File \"/usr/lib/python3.10/socket.py\", line 705, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 381, in signal_handler\n",
            "    self.cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/context.py\", line 2446, in cancelAllJobs\n",
            "    self._jsc.sc().cancelAllJobs()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1322, in __call__\n",
            "    return_value = get_return_value(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
            "    return f(*a, **kw)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/protocol.py\", line 334, in get_return_value\n",
            "    raise Py4JError(\n",
            "py4j.protocol.Py4JError: An error occurred while calling o655.sc\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
            "    response = connection.send_command(command)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/py4j/clientserver.py\", line 539, in send_command\n",
            "    raise Py4JNetworkError(\n",
            "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
            " 75%|███████▍  | 91.8M/123M [00:05<00:03, 8.93MiB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to process file /path/to/github_events/downloaded/2024-05-01/2024-05-01-7.json.gz for 2024-05-01: An error occurred while calling o1028.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 91%|█████████▏| 113M/123M [00:07<00:00, 11.1MiB/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-98c3621ce243>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocess_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles_by_day\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_directory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-27-877ce4b8662c>\u001b[0m in \u001b[0;36mprocess_files\u001b[0;34m(files_by_day, downloaded_directory, processed_directory, max_retries, retry_delay)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Validate JSON file before processing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_valid_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Skipping corrupted file {file_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-56af27f8cc47>\u001b[0m in \u001b[0;36mis_valid_json\u001b[0;34m(filepath)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                 \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \"\"\"\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m    352\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/context.py\u001b[0m in \u001b[0;36msignal_handler\u001b[0;34m(signal, frame)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0msignal_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNoReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcancelAllJobs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# see http://stackoverflow.com/questions/23206787/\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 93%|█████████▎| 114M/123M [00:07<00:00, 11.8MiB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(zip_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "iF4RjZpHd4tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def zip_all_subdirectories(output_directory, zip_directory):\n",
        "    \"\"\"\n",
        "    Zips all subdirectories within the specified output directory.\n",
        "\n",
        "    This function traverses the specified output directory, zipping all subdirectories\n",
        "    that contain files. Each zip file is saved in a corresponding subdirectory within\n",
        "    the specified zip directory. The zip files are named based on the relative path\n",
        "    of the subdirectory, with '/' replaced by '_'.\n",
        "\n",
        "    Parameters:\n",
        "    output_directory (str): The base directory containing the subdirectories to be zipped.\n",
        "    zip_directory (str): The base directory where the zip files will be saved.\n",
        "\n",
        "    Prints:\n",
        "    Progress messages, including skipping existing zip files and any errors encountered\n",
        "    during the zipping process.\n",
        "\n",
        "    \"\"\"\n",
        "    for root, dirs, files in os.walk(output_directory):\n",
        "        # Process each subdirectory containing files\n",
        "        if files:\n",
        "            subdirectory_name = os.path.relpath(root, output_directory)\n",
        "            zip_file_name = f\"{subdirectory_name.replace('/', '_')}.zip\"\n",
        "\n",
        "            # Create the daily subdirectory in the base zip directory\n",
        "            date_subdirectory = os.path.join(zip_directory, subdirectory_name)\n",
        "            os.makedirs(date_subdirectory, exist_ok=True)\n",
        "\n",
        "            zip_file_path = os.path.join(date_subdirectory, zip_file_name)\n",
        "\n",
        "            # Check if the zip file already exists\n",
        "            if os.path.exists(zip_file_path):\n",
        "                print(f\"Zip file {zip_file_path} already exists. Skipping...\")\n",
        "                continue\n",
        "\n",
        "            # Create the zip file\n",
        "            try:\n",
        "                with ZipFile(zip_file_path, 'w') as zipf:\n",
        "                    for file in files:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        zipf.write(file_path, os.path.relpath(file_path, output_directory))\n",
        "                print(f\"Zipped {root} to {zip_file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while creating the zip file: {e}\")"
      ],
      "metadata": {
        "id": "UCPRsJqtUUUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_all_subdirectories(output_directory, zip_directory)"
      ],
      "metadata": {
        "id": "r9ynJE8QZPsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_download_links(zip_directory):\n",
        "    for root, dirs, files in os.walk(zip_directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.zip'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                display(FileLink(file_path))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "x0rrtGm8UV4U",
        "outputId": "4d847bb3-6b2e-4e9f-bb9d-4b417d8ac936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/path/to/github_events/zipped/user_agg_2024-05-02.csv/user_agg_2024-05-02.csv.zip"
            ],
            "text/html": [
              "<a href='/path/to/github_events/zipped/user_agg_2024-05-02.csv/user_agg_2024-05-02.csv.zip' target='_blank'>/path/to/github_events/zipped/user_agg_2024-05-02.csv/user_agg_2024-05-02.csv.zip</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/path/to/github_events/zipped/user_agg_2024-05-02.parquet/user_agg_2024-05-02.parquet.zip"
            ],
            "text/html": [
              "<a href='/path/to/github_events/zipped/user_agg_2024-05-02.parquet/user_agg_2024-05-02.parquet.zip' target='_blank'>/path/to/github_events/zipped/user_agg_2024-05-02.parquet/user_agg_2024-05-02.parquet.zip</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/path/to/github_events/zipped/user_agg_2024-05-01.csv/user_agg_2024-05-01.csv.zip"
            ],
            "text/html": [
              "<a href='/path/to/github_events/zipped/user_agg_2024-05-01.csv/user_agg_2024-05-01.csv.zip' target='_blank'>/path/to/github_events/zipped/user_agg_2024-05-01.csv/user_agg_2024-05-01.csv.zip</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/path/to/github_events/zipped/repo_agg_2024-05-01.csv/repo_agg_2024-05-01.csv.zip"
            ],
            "text/html": [
              "<a href='/path/to/github_events/zipped/repo_agg_2024-05-01.csv/repo_agg_2024-05-01.csv.zip' target='_blank'>/path/to/github_events/zipped/repo_agg_2024-05-01.csv/repo_agg_2024-05-01.csv.zip</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/path/to/github_events/zipped/repo_agg_2024-05-02.parquet/repo_agg_2024-05-02.parquet.zip"
            ],
            "text/html": [
              "<a href='/path/to/github_events/zipped/repo_agg_2024-05-02.parquet/repo_agg_2024-05-02.parquet.zip' target='_blank'>/path/to/github_events/zipped/repo_agg_2024-05-02.parquet/repo_agg_2024-05-02.parquet.zip</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/path/to/github_events/zipped/repo_agg_2024-05-01.parquet/repo_agg_2024-05-01.parquet.zip"
            ],
            "text/html": [
              "<a href='/path/to/github_events/zipped/repo_agg_2024-05-01.parquet/repo_agg_2024-05-01.parquet.zip' target='_blank'>/path/to/github_events/zipped/repo_agg_2024-05-01.parquet/repo_agg_2024-05-01.parquet.zip</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/path/to/github_events/zipped/user_agg_2024-05-01.parquet/user_agg_2024-05-01.parquet.zip"
            ],
            "text/html": [
              "<a href='/path/to/github_events/zipped/user_agg_2024-05-01.parquet/user_agg_2024-05-01.parquet.zip' target='_blank'>/path/to/github_events/zipped/user_agg_2024-05-01.parquet/user_agg_2024-05-01.parquet.zip</a><br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "/path/to/github_events/zipped/repo_agg_2024-05-02.csv/repo_agg_2024-05-02.csv.zip"
            ],
            "text/html": [
              "<a href='/path/to/github_events/zipped/repo_agg_2024-05-02.csv/repo_agg_2024-05-02.csv.zip' target='_blank'>/path/to/github_events/zipped/repo_agg_2024-05-02.csv/repo_agg_2024-05-02.csv.zip</a><br>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display_download_links(zip_directory)"
      ],
      "metadata": {
        "id": "xRaXiLz7hZpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11hql4SjUiW6tiSx83PouYaFnfZKsAWEj",
      "authorship_tag": "ABX9TyO+1we0AvplS7ejE73ftF4V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}