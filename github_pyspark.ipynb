{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mrncstt/githubevents_pypsark/blob/main/github_pyspark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGuEGnvKq7v4"
      },
      "outputs": [],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBUPhFMFrBi5"
      },
      "outputs": [],
      "source": [
        "!python -m pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "e0g0lYl8rDNZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import requests\n",
        "import calendar\n",
        "from zipfile import ZipFile\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from requests.exceptions import RequestException, HTTPError, ConnectionError, Timeout\n",
        "from tqdm import tqdm\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, count, to_date, when\n",
        "from IPython.display import FileLink, display\n",
        "import gzip\n",
        "import json\n",
        "import time\n",
        "import py4j\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "fwJiCre3amzb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "5JaT529l14tv"
      },
      "outputs": [],
      "source": [
        "def create_directory(path):\n",
        "    \"\"\"\n",
        "    Creates a directory if it doesn't exist.\n",
        "\n",
        "    Parameters:\n",
        "    path (str): The path of the directory to be created.\n",
        "\n",
        "    Logs:\n",
        "    - Info: If the directory is created successfully or already exists.\n",
        "    - Error: If an error occurs during the directory creation process.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        path_obj = Path(path)\n",
        "        if not path_obj.exists():\n",
        "            path_obj.mkdir(parents=True, exist_ok=True)\n",
        "            logging.info(f\"Directory '{path}' created successfully.\")\n",
        "        else:\n",
        "            logging.info(f\"Directory '{path}' already exists.\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"An error occurred while creating the directory '{path}': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0SXCRpdK16Bu"
      },
      "outputs": [],
      "source": [
        "def download_file(url, output_file):\n",
        "    \"\"\"\n",
        "    Downloads a file from a URL to the specified output path.\n",
        "\n",
        "    Parameters:\n",
        "    url (str): The URL of the file to be downloaded.\n",
        "    output_file (str): The path where the downloaded file will be saved.\n",
        "\n",
        "    Logs:\n",
        "    - Info: If the file is downloaded successfully.\n",
        "    - Error: If an error occurs during the download process, including HTTP errors, connection errors, and timeouts.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with requests.get(url, stream=True, timeout=10) as response:\n",
        "            response.raise_for_status()\n",
        "            total_size = int(response.headers.get('content-length', 0))\n",
        "            block_size = 1024\n",
        "            t = tqdm(total=total_size, unit='iB', unit_scale=True)\n",
        "\n",
        "            with open(output_file, 'wb') as f:\n",
        "                for data in response.iter_content(block_size):\n",
        "                    t.update(len(data))\n",
        "                    f.write(data)\n",
        "            t.close()\n",
        "\n",
        "        logging.info(f\"Downloaded - {url}\\nPath - {output_file}\")\n",
        "    except (requests.HTTPError, requests.ConnectionError, requests.Timeout) as e:\n",
        "        logging.error(f\"Error occurred while downloading {url}: {e}\")\n",
        "    except requests.RequestException as e:\n",
        "        logging.error(f\"Request error occurred while downloading {url}: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "-IwMEgGjH6BB"
      },
      "outputs": [],
      "source": [
        "def download_github_events(year, month, output_dir):\n",
        "    \"\"\"\n",
        "    Downloads GitHub event data for the specified year and month to the given output directory.\n",
        "\n",
        "    Parameters:\n",
        "    year (int): The year of the event data to download.\n",
        "    month (int): The month of the event data to download.\n",
        "    output_dir (str): The directory where the downloaded data will be saved.\n",
        "\n",
        "    Creates directories for each day and hour if they don't exist, and downloads the data files. Uses parallel processing to speed up the download process.\n",
        "    \"\"\"\n",
        "    base_url = \"https://data.gharchive.org/\"\n",
        "    create_directory(output_dir)\n",
        "\n",
        "    num_days = calendar.monthrange(year, month)[1]\n",
        "\n",
        "    def download_day_hour(day, hour):\n",
        "        \"\"\"Downloads the GitHub event data for a specific day and hour.\"\"\"\n",
        "        day_dir = os.path.join(output_dir, f\"{year}-{month:02d}-{day:02d}\")\n",
        "        create_directory(day_dir)\n",
        "\n",
        "        url = f\"{base_url}{year}-{month:02d}-{day:02d}-{hour}.json.gz\"\n",
        "        output_file = os.path.join(day_dir, f\"{year}-{month:02d}-{day:02d}-{hour}.json.gz\")\n",
        "        if not os.path.exists(output_file):\n",
        "            logging.info(f\"Downloading {url} to {output_file}\")\n",
        "            download_file(url, output_file)\n",
        "        else:\n",
        "            logging.info(f\"File already exists: {output_file}\")\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        for day in range(1, num_days + 1):\n",
        "            for hour in range(24):\n",
        "                executor.submit(download_day_hour, day, hour)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ll4DmmMw-P-t"
      },
      "outputs": [],
      "source": [
        "def verify_downloaded_files(year, month, output_dir):\n",
        "    \"\"\"\n",
        "    Verifies that all expected files for a given month and year have been downloaded.\n",
        "\n",
        "    This function checks for the presence of JSON files compressed with gzip (.json.gz)\n",
        "    for every hour of every day in the specified month and year. It assumes the files are\n",
        "    organized in a directory structure where each day has its own subdirectory named\n",
        "    in the format 'YYYY-MM-DD', and each file is named in the format 'YYYY-MM-DD-HH.json.gz'.\n",
        "\n",
        "    Parameters:\n",
        "    year (int): The year for which to verify the files.\n",
        "    month (int): The month for which to verify the files (1-12).\n",
        "    output_dir (str): The base directory where the files are expected to be located.\n",
        "\n",
        "    Prints:\n",
        "    A list of missing files if any are not found, otherwise a confirmation message that all files are present.\n",
        "    \"\"\"\n",
        "    missing_files = []\n",
        "    num_days = calendar.monthrange(year, month)[1]\n",
        "\n",
        "    for day in range(1, num_days + 1):\n",
        "        for hour in range(24):\n",
        "            expected_file = os.path.join(output_dir, f\"{year}-{month:02d}-{day:02d}\", f\"{year}-{month:02d}-{day:02d}-{hour}.json.gz\")\n",
        "            if not os.path.exists(expected_file):\n",
        "                missing_files.append(expected_file)\n",
        "\n",
        "    if missing_files:\n",
        "        print(f\"Some files are missing ({len(missing_files)} files):\")\n",
        "        for file in missing_files:\n",
        "            print(file)\n",
        "    else:\n",
        "        print(\"All files have been downloaded successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zljwR1eU-RsN"
      },
      "outputs": [],
      "source": [
        "year = 2024\n",
        "month = 5\n",
        "output_dir = '/path/to/github_events/downloaded'\n",
        "output_directory = '/path/to/github_events/processed'\n",
        "zip_directory = '/path/to/github_events/zipped'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_p9SGlCeGthb"
      },
      "outputs": [],
      "source": [
        "download_github_events(year, month, output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS6eMVk6EmoQ",
        "outputId": "7cdaab47-be79-46cc-c68b-7ade933ca58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2024-05-01, Files processed: 24\n",
            "Date: 2024-05-02, Files processed: 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 41%|████      | 39.2M/96.2M [00:00<00:01, 42.8MiB/s]"
          ]
        }
      ],
      "source": [
        "def read_files(output_dir):\n",
        "    \"\"\"\n",
        "    Reads all .json.gz files from the specified directory and groups them by day.\n",
        "\n",
        "    This function traverses through the specified directory and its subdirectories,\n",
        "    collects all .json.gz files, and groups them by their date, assuming that each subdirectory\n",
        "    is named in the format 'YYYY-MM-DD'. It returns a dictionary where the keys are dates and the\n",
        "    values are lists of file paths.\n",
        "\n",
        "    Parameters:\n",
        "    output_dir (str): The base directory where the files are expected to be located.\n",
        "\n",
        "    Returns:\n",
        "    dict: A dictionary where the keys are dates (str) and the values are lists of file paths (str).\n",
        "\n",
        "    Prints:\n",
        "    A message indicating the number of files processed per day. If the directory does not exist,\n",
        "    it prints an error message.\n",
        "\n",
        "    \"\"\"\n",
        "    if not os.path.exists(output_dir):\n",
        "        print(f\"Directory {output_dir} does not exist.\")\n",
        "        return {}\n",
        "\n",
        "    files_by_day = {}\n",
        "\n",
        "    for root, _, files in os.walk(output_dir):\n",
        "        for file in files:\n",
        "            if file.endswith(\".json.gz\"):\n",
        "                try:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    file_date = root.split('/')[-1]\n",
        "                    if file_date not in files_by_day:\n",
        "                        files_by_day[file_date] = []\n",
        "                    files_by_day[file_date].append(file_path)\n",
        "                except IndexError:\n",
        "                    print(f\"Skipping file with unexpected format: {file_path}\")\n",
        "\n",
        "    for date, files in files_by_day.items():\n",
        "        print(f\"Date: {date}, Files processed: {len(files)}\")\n",
        "\n",
        "    return files_by_day\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files_by_day = read_files(output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19NlwSeQeZ2x",
        "outputId": "1afb6a00-840b-4c3f-ef2c-4f8b15198856"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date: 2024-05-03, Files processed: 7\n",
            "Date: 2024-05-01, Files processed: 24\n",
            "Date: 2024-05-02, Files processed: 24\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 66%|██████▌   | 59.6M/90.3M [00:01<00:00, 48.0MiB/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_json(filepath):\n",
        "    \"\"\"\n",
        "    Checks if a gzip-compressed JSON file is valid.\n",
        "\n",
        "    This function attempts to open and read a gzip-compressed file line by line,\n",
        "    parsing each line as JSON. If all lines can be successfully parsed, the file\n",
        "    is considered valid. If any line fails to parse as JSON or if the file cannot\n",
        "    be read due to an EOFError, the file is considered invalid.\n",
        "\n",
        "    Parameters:\n",
        "    filepath (str): The path to the gzip-compressed JSON file.\n",
        "\n",
        "    Returns:\n",
        "    bool: True if the file is valid JSON, False otherwise.\n",
        "\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with gzip.open(filepath, 'rt', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                json.loads(line)\n",
        "        return True\n",
        "    except (json.JSONDecodeError, EOFError):\n",
        "        return False"
      ],
      "metadata": {
        "id": "4DLsdT-PSSyi"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_spark_session(max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Creates a Spark session with retry logic.\n",
        "\n",
        "    This function attempts to create a Spark session using the given number of retries and delay between retries.\n",
        "    If the Spark session cannot be created due to a Py4JNetworkError, it retries up to the specified maximum number of retries,\n",
        "    waiting for the specified delay between each attempt. If it fails to create the Spark session after the specified number of retries,\n",
        "    it raises an exception.\n",
        "\n",
        "    Parameters:\n",
        "    max_retries (int): The maximum number of retry attempts (default is 3).\n",
        "    retry_delay (int): The delay in seconds between retry attempts (default is 5 seconds).\n",
        "\n",
        "    Returns:\n",
        "    SparkSession: A SparkSession object if the session is successfully created.\n",
        "\n",
        "    Raises:\n",
        "    Exception: If the Spark session cannot be created after the specified number of retries.\n",
        "\n",
        "    \"\"\"\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        try:\n",
        "            spark = SparkSession.builder.appName(\"GitHub\").getOrCreate()\n",
        "            return spark\n",
        "        except py4j.protocol.Py4JNetworkError as e:\n",
        "            retries += 1\n",
        "            print(f\"Retry {retries}/{max_retries} - Failed to create Spark session: {e}\")\n",
        "            time.sleep(retry_delay)\n",
        "    raise Exception(\"Failed to create Spark session after multiple retries\")"
      ],
      "metadata": {
        "id": "I3bs6-8kSUYp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "rvEznQ_RDgLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df64b1bf-8ade-452e-e0f4-00b2926bdf27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|          | 408k/53.9M [00:00<00:13, 4.05MiB/s]"
          ]
        }
      ],
      "source": [
        "def process_files(files_by_day, downloaded_directory, processed_directory, max_retries=3, retry_delay=5):\n",
        "    \"\"\"\n",
        "    Processes JSON files grouped by day, performing aggregations and saving results.\n",
        "\n",
        "    This function processes JSON files grouped by their date, performing specific aggregations for repositories and users.\n",
        "    The results are saved as CSV and Parquet files. It includes retry logic for handling failures during Spark session creation\n",
        "    and file processing.\n",
        "\n",
        "    Parameters:\n",
        "    files_by_day (dict): A dictionary where the keys are dates (str) and the values are lists of file paths (str).\n",
        "    downloaded_directory (str): The directory where the downloaded JSON files are located.\n",
        "    processed_directory (str): The directory where the processed CSV and Parquet files will be saved.\n",
        "    max_retries (int): The maximum number of retry attempts for creating the Spark session and processing each file (default is 3).\n",
        "    retry_delay (int): The delay in seconds between retry attempts (default is 5 seconds).\n",
        "\n",
        "    Prints:\n",
        "    Progress and error messages, including retries and skipping of corrupted files.\n",
        "    \"\"\"\n",
        "    os.makedirs(processed_directory, exist_ok=True)\n",
        "\n",
        "    for file_date, files in files_by_day.items():\n",
        "        print(f\"Processing files for {file_date}...\")\n",
        "\n",
        "        repo_output_csv = os.path.join(processed_directory, f\"repo_agg_{file_date}.csv\")\n",
        "        repo_output_parquet = os.path.join(processed_directory, f\"repo_agg_{file_date}.parquet\")\n",
        "        user_output_csv = os.path.join(processed_directory, f\"user_agg_{file_date}.csv\")\n",
        "        user_output_parquet = os.path.join(processed_directory, f\"user_agg_{file_date}.parquet\")\n",
        "\n",
        "        if (os.path.exists(repo_output_csv) and os.path.exists(repo_output_parquet) and\n",
        "            os.path.exists(user_output_csv) and os.path.exists(user_output_parquet)):\n",
        "            print(f\"Output files for {file_date} already exist. Skipping processing.\")\n",
        "            continue\n",
        "\n",
        "        spark = create_spark_session(max_retries, retry_delay)\n",
        "\n",
        "        for file in files:\n",
        "            file_path = os.path.join(downloaded_directory, file)\n",
        "\n",
        "            if not is_valid_json(file_path):\n",
        "                print(f\"Skipping corrupted file {file_path}\")\n",
        "                continue\n",
        "\n",
        "            retries = 0\n",
        "            success = False\n",
        "\n",
        "            while retries < max_retries and not success:\n",
        "                try:\n",
        "                    df = spark.read.json(file_path)\n",
        "\n",
        "                    repo_df = df.select(to_date(col(\"created_at\")).alias(\"date\"),\n",
        "                                        col(\"repo.id\").alias(\"project_id\"),\n",
        "                                        col(\"repo.name\").alias(\"project_name\"),\n",
        "                                        col(\"type\"))\n",
        "\n",
        "                    user_df = df.select(to_date(col(\"created_at\")).alias(\"date\"),\n",
        "                                        col(\"actor.id\").alias(\"user_id\"),\n",
        "                                        col(\"actor.login\").alias(\"user_login\"),\n",
        "                                        col(\"type\"))\n",
        "\n",
        "                    repo_agg = repo_df.groupBy(\"date\", \"project_id\", \"project_name\").agg(\n",
        "                        count(when(col(\"type\") == \"WatchEvent\", True)).alias(\"stars\"),\n",
        "                        count(when(col(\"type\") == \"ForkEvent\", True)).alias(\"forks\"),\n",
        "                        count(when(col(\"type\") == \"IssuesEvent\", True)).alias(\"issues\"),\n",
        "                        count(when(col(\"type\") == \"PullRequestEvent\", True)).alias(\"prs\")\n",
        "                    )\n",
        "\n",
        "                    user_agg = user_df.groupBy(\"date\", \"user_id\", \"user_login\").agg(\n",
        "                        count(when(col(\"type\") == \"WatchEvent\", True)).alias(\"starred_projects\"),\n",
        "                        count(when(col(\"type\") == \"IssuesEvent\", True)).alias(\"issues_created\"),\n",
        "                        count(when(col(\"type\") == \"PullRequestEvent\", True)).alias(\"prs_created\")\n",
        "                    )\n",
        "\n",
        "                    repo_agg.write.csv(repo_output_csv, header=True, mode='overwrite')\n",
        "                    repo_agg.write.parquet(repo_output_parquet, mode='overwrite')\n",
        "                    user_agg.write.csv(user_output_csv, header=True, mode='overwrite')\n",
        "                    user_agg.write.parquet(user_output_parquet, mode='overwrite')\n",
        "\n",
        "                    print(f\"Processed and saved data for file {file}\")\n",
        "                    success = True\n",
        "                except py4j.protocol.Py4JJavaError as e:\n",
        "                    retries += 1\n",
        "                    print(f\"Retry {retries}/{max_retries} - Failed to process file {file} for {file_date} due to Java error: {e}\")\n",
        "                    time.sleep(retry_delay)\n",
        "                except py4j.protocol.Py4JNetworkError as e:\n",
        "                    retries += 1\n",
        "                    print(f\"Retry {retries}/{max_retries} - Failed to process file {file} for {file_date} due to network error: {e}\")\n",
        "                    time.sleep(retry_delay)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to process file {file} for {file_date}: {e}\")\n",
        "                    break\n",
        "\n",
        "        spark.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "process_files(files_by_day, output_dir, output_directory)"
      ],
      "metadata": {
        "id": "jM5dg2XNQ28F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.makedirs(zip_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "iF4RjZpHd4tO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def zip_all_subdirectories(output_directory, zip_directory):\n",
        "    \"\"\"\n",
        "    Zips all subdirectories within the specified output directory.\n",
        "\n",
        "    This function traverses the specified output directory, zipping all subdirectories\n",
        "    that contain files. Each zip file is saved in a corresponding subdirectory within\n",
        "    the specified zip directory. The zip files are named based on the relative path\n",
        "    of the subdirectory, with '/' replaced by '_'.\n",
        "\n",
        "    Parameters:\n",
        "    output_directory (str): The base directory containing the subdirectories to be zipped.\n",
        "    zip_directory (str): The base directory where the zip files will be saved.\n",
        "\n",
        "    Prints:\n",
        "    Progress messages, including skipping existing zip files and any errors encountered\n",
        "    during the zipping process.\n",
        "\n",
        "    \"\"\"\n",
        "    for root, dirs, files in os.walk(output_directory):\n",
        "        if files:\n",
        "            subdirectory_name = os.path.relpath(root, output_directory)\n",
        "            zip_file_name = f\"{subdirectory_name.replace('/', '_')}.zip\"\n",
        "\n",
        "            date_subdirectory = os.path.join(zip_directory, subdirectory_name)\n",
        "            os.makedirs(date_subdirectory, exist_ok=True)\n",
        "\n",
        "            zip_file_path = os.path.join(date_subdirectory, zip_file_name)\n",
        "\n",
        "            if os.path.exists(zip_file_path):\n",
        "                print(f\"Zip file {zip_file_path} already exists. Skipping...\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                with ZipFile(zip_file_path, 'w') as zipf:\n",
        "                    for file in files:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        zipf.write(file_path, os.path.relpath(file_path, output_directory))\n",
        "                print(f\"Zipped {root} to {zip_file_path}\")\n",
        "            except Exception as e:\n",
        "                print(f\"An error occurred while creating the zip file: {e}\")"
      ],
      "metadata": {
        "id": "UCPRsJqtUUUe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "zip_all_subdirectories(output_directory, zip_directory)"
      ],
      "metadata": {
        "id": "r9ynJE8QZPsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_download_links(zip_directory):\n",
        "    for root, dirs, files in os.walk(zip_directory):\n",
        "        for file in files:\n",
        "            if file.endswith('.zip'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                display(FileLink(file_path))"
      ],
      "metadata": {
        "id": "x0rrtGm8UV4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_download_links(zip_directory)"
      ],
      "metadata": {
        "id": "xRaXiLz7hZpJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "11hql4SjUiW6tiSx83PouYaFnfZKsAWEj",
      "authorship_tag": "ABX9TyNSyCyFLc2avjzFGm9NQo1r",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}